# Import packages
import pandas as pd
import numpy as np


# Then, load the dataset into a dataframe. Creating a dataframe will help you conduct data manipulation, exploratory data analysis (EDA), and statistical activities.
# 
# **Note:** As shown in this cell, the dataset has been automatically loaded in for you. You do not need to download the .csv file, or provide more code, in order to access the dataset and proceed with this lab. Please continue with this activity by completing the following instructions.

# In[9]:


# Load dataset into dataframe
data = pd.read_csv("tiktok_dataset.csv")


# ### **Task 2b. Understand the data - Inspect the data**
# 
# View and inspect summary information about the dataframe by **coding the following:**
# 
# 1. `data.head(10)`
# 2. `data.info()`
# 3. `data.describe()`
# 
# *Consider the following questions:*
# 
# **Question 1:** When reviewing the first few rows of the dataframe, what do you observe about the data? What does each row represent?
# 
# **Question 2:** When reviewing the `data.info()` output, what do you notice about the different variables? Are there any null values? Are all of the variables numeric? Does anything else stand out?
# 
# **Question 3:** When reviewing the `data.describe()` output, what do you notice about the distributions of each variable? Are there any questionable values? Does it seem that there are outlier values?
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 

# In[5]:


# Display and examine the first ten rows of the dataframe
data.head(10)


# In[4]:


# Get summary info
data.info()


# In[9]:


# Get summary statistics
data.describe()


# Answer 1). There are details mentioned for each video such as like,share,download,view and comments, this will help in accessing the reach of video to the audience.
# 
# Answer 2). There exist null values for column video_id,video_transcription_text,video_view_count,video_like_count,video_share_count,video_download_count,video_comment_count.
# 
# Answer 3). There are some outliers in the data at first look from summary, and maximum view,like,share,comment is generated by 25% of the data.

# ### **Task 2c. Understand the data - Investigate the variables**
# 
# In this phase, you will begin to investigate the variables more closely to better understand them.
# 
# You know from the project proposal that the ultimate objective is to use machine learning to classify videos as either claims or opinions. A good first step towards understanding the data might therefore be examining the `claim_status` variable. Begin by determining how many videos there are for each different claim status.

# In[15]:


# What are the different values for claim status and how many of each are in the data
data.groupby('claim_status').size()


# **Question:** What do you notice about the values shown?

# **Response:** Claim and Opinions are balanced in the number.

# Next, examine the engagement trends associated with each different claim status.
# 
# Start by using Boolean masking to filter the data according to claim status, then calculate the mean and median view counts for each claim status.

# In[17]:


# What is the average view count of videos with "claim" status?

mask=data['claim_status']=='claim'
print("Mean:",data[mask]['video_view_count'].mean())
print("Median",data[mask]['video_view_count'].median())


# In[18]:


# What is the average view count of videos with "opinion" status?
### YOUR CODE HERE ###
mask_opinion=data['claim_status']=='opinion'
print("Mean:",data[mask_opinion]['video_view_count'].mean())
print("Median:",data[mask_opinion]['video_view_count'].median())


# **Question:** What do you notice about the mean and media within each claim category?
# 
# Now, examine trends associated with the ban status of the author.
# 
# Use `groupby()` to calculate how many videos there are for each combination of categories of claim status and author ban status.

# **Response:** Views are significantly higher for claim status than the opinion status. Mean and Median is almost same for each category.

# In[40]:


# Get counts for each group combination of claim status and author ban status
### YOUR CODE HERE ###
pd.crosstab(data.claim_status,data.author_ban_status)


# **Question:** What do you notice about the number of claims videos with banned authors? Why might this relationship occur?
# 
# Continue investigating engagement levels, now focusing on `author_ban_status`.
# 
# Calculate the median video share count of each author ban status.

# **Response:** Authors are banned more for the video with claim status and large number is under review as well than opinion based video.

# In[30]:


## Deleting Null values from the data.
data.isna().sum()


# In[42]:


# What's the median video share count of each author ban status?
data.groupby('author_ban_status')['video_share_count'].median()


# **Question:** What do you notice about the share count of banned authors, compared to that of active authors? Explore this in more depth.
# 
# Use `groupby()` to group the data by `author_ban_status`, then use `agg()` to get the count, mean, and median of each of the following columns:
# * `video_view_count`
# * `video_like_count`
# * `video_share_count`
# 
# Remember, the argument for the `agg()` function is a dictionary whose keys are columns. The values for each column are a list of the calculations you want to perform.

# In[46]:


data.groupby('author_ban_status').agg({
    'video_view_count': ['mean','median','count'],
    'video_like_count':['mean','median','count'],
    'video_share_count':['mean','median','count']
})


# **Question:** What do you notice about the number of views, likes, and shares for banned authors compared to active authors?
# 
# Now, create three new columns to help better understand engagement rates:
# * `likes_per_view`: represents the number of likes divided by the number of views for each video
# * `comments_per_view`: represents the number of comments divided by the number of views for each video
# * `shares_per_view`: represents the number of shares divided by the number of views for each video

# In[35]:


# Create a likes_per_view column
data['likes_per_view']=data['video_like_count']/data['video_view_count']

# Create a comments_per_view column
data['comment_per_view']=data['video_comment_count']/data['video_view_count']

# Create a shares_per_view column
data['shares_per_view']=data['video_share_count']/data['video_view_count']


# Use `groupby()` to compile the information in each of the three newly created columns for each combination of categories of claim status and author ban status, then use `agg()` to calculate the count, the mean, and the median of each group.

# In[37]:


data.groupby(['claim_status','author_ban_status']).agg({'likes_per_view':['mean','median','count'],
                                                     'comment_per_view':['mean','median','count'],
                                                      'shares_per_view':['mean','median','count']})
